{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9607a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (700000, 26)\n",
      "Test shape: (300000, 25)\n",
      "\n",
      "--- Train Data Head ---\n",
      "   id  age  alcohol_consumption_per_week  physical_activity_minutes_per_week  \\\n",
      "0   0   31                             1                                  45   \n",
      "1   1   50                             2                                  73   \n",
      "2   2   32                             3                                 158   \n",
      "3   3   54                             3                                  77   \n",
      "4   4   54                             1                                  55   \n",
      "\n",
      "   diet_score  sleep_hours_per_day  screen_time_hours_per_day   bmi  \\\n",
      "0         7.7                  6.8                        6.1  33.4   \n",
      "1         5.7                  6.5                        5.8  23.8   \n",
      "2         8.5                  7.4                        9.1  24.1   \n",
      "3         4.6                  7.0                        9.2  26.6   \n",
      "4         5.7                  6.2                        5.1  28.8   \n",
      "\n",
      "   waist_to_hip_ratio  systolic_bp  ...  gender  ethnicity  education_level  \\\n",
      "0                0.93          112  ...  Female   Hispanic       Highschool   \n",
      "1                0.83          120  ...  Female      White       Highschool   \n",
      "2                0.83           95  ...    Male   Hispanic       Highschool   \n",
      "3                0.83          121  ...  Female      White       Highschool   \n",
      "4                0.90          108  ...    Male      White       Highschool   \n",
      "\n",
      "   income_level  smoking_status  employment_status family_history_diabetes  \\\n",
      "0  Lower-Middle         Current           Employed                       0   \n",
      "1  Upper-Middle           Never           Employed                       0   \n",
      "2  Lower-Middle           Never            Retired                       0   \n",
      "3  Lower-Middle         Current           Employed                       0   \n",
      "4  Upper-Middle           Never            Retired                       0   \n",
      "\n",
      "  hypertension_history cardiovascular_history diagnosed_diabetes  \n",
      "0                    0                      0                1.0  \n",
      "1                    0                      0                1.0  \n",
      "2                    0                      0                0.0  \n",
      "3                    1                      0                1.0  \n",
      "4                    1                      0                1.0  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "--- Submission Format Example ---\n",
      "       id  diagnosed_diabetes\n",
      "0  700000                   0\n",
      "1  700001                   0\n",
      "2  700002                   0\n",
      "3  700003                   0\n",
      "4  700004                   0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Load the datasets\n",
    "# Assuming the files are in the same directory as your script/notebook\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "submission_df = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 2. Basic sanity check - Print the size of the datasets\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# 3. Look at the first few rows to understand the features\n",
    "print(\"\\n--- Train Data Head ---\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n--- Submission Format Example ---\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123bac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Separate Target and ID\n",
    "target = 'diagnosed_diabetes'\n",
    "# Drop rows in train where target might be missing (just in case)\n",
    "train_df = train_df.dropna(subset=[target]) \n",
    "\n",
    "y = train_df[target].values # The labels for training\n",
    "train_ids = train_df['id']\n",
    "test_ids = test_df['id']\n",
    "\n",
    "# 2. Drop unnecessary columns\n",
    "# We drop 'id' because it's just an index, not a feature\n",
    "# We drop 'diagnosed_diabetes' from train_df to match test_df structure for processing\n",
    "train_features = train_df.drop(['id', 'diagnosed_diabetes'], axis=1)\n",
    "test_features = test_df.drop(['id'], axis=1)\n",
    "\n",
    "# 3. Combine temporarily for consistent preprocessing\n",
    "# This ensures that if 'test' has a category 'train' doesn't (or vice versa), the columns still match\n",
    "all_features = pd.concat([train_features, test_features], axis=0)\n",
    "\n",
    "# --- IDENTIFY COLUMNS ---\n",
    "\n",
    "# Numerical columns (Continuous values)\n",
    "numerical_cols = [\n",
    "    'age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week',\n",
    "    'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day',\n",
    "    'bmi', 'waist_to_hip_ratio', 'systolic_bp'\n",
    "]\n",
    "\n",
    "# Categorical columns (Text/Strings)\n",
    "categorical_cols = [\n",
    "    'gender', 'ethnicity', 'education_level', \n",
    "    'income_level', 'smoking_status', 'employment_status'\n",
    "]\n",
    "\n",
    "# Binary/Already Numeric columns (0/1) - We usually leave these alone\n",
    "# (family_history_diabetes, hypertension_history, cardiovascular_history)\n",
    "\n",
    "# --- PREPROCESSING ---\n",
    "\n",
    "# A. Handle Categorical Data (One-Hot Encoding)\n",
    "# drop_first=True helps reduce redundancy (e.g., if is_Male=0, we know is_Female=1)\n",
    "all_features = pd.get_dummies(all_features, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# B. Handle Numerical Data (Scaling)\n",
    "scaler = StandardScaler()\n",
    "all_features[numerical_cols] = scaler.fit_transform(all_features[numerical_cols])\n",
    "\n",
    "# C. Handle Missing Values (Simple Imputation)\n",
    "# Fill numeric NaNs with Mean, others with 0\n",
    "all_features[numerical_cols] = all_features[numerical_cols].fillna(all_features[numerical_cols].mean())\n",
    "all_features = all_features.fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e65baa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Input Shape: (560000, 36)\n",
      "Target Shape: (560000,)\n",
      "Validation Shape: (140000, 36)\n"
     ]
    }
   ],
   "source": [
    "# --- SPLIT BACK TO TRAIN / TEST ---\n",
    "\n",
    "# Split back using the original length of the train dataframe\n",
    "X = all_features.iloc[:len(train_df)].values\n",
    "X_kaggle_test = all_features.iloc[len(train_df):].values\n",
    "\n",
    "# --- CREATE VALIDATION SET ---\n",
    "# Essential for Neural Nets to stop training before overfitting\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Final Input Shape: {X_train.shape}\")\n",
    "print(f\"Target Shape: {y_train.shape}\")\n",
    "print(f\"Validation Shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d4e9820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for non-numeric columns...\n",
      "         0         1         2         3         4         5         6   \\\n",
      "0 -0.373175 -0.073644 -0.106648  0.368804 -1.657032  0.092278  0.635062   \n",
      "1  0.989543   0.87543 -0.379345 -0.380158  1.213476  1.468802  0.844062   \n",
      "2 -1.565553 -0.073644 -0.197547 -1.537644 -0.994607  1.173833  0.112563   \n",
      "3  0.819204 -1.022719 -0.233907  0.232629  0.551051  1.321318 -0.758268   \n",
      "4  0.734034 -0.073644 -0.342985   2.34334 -0.884203  0.928025  0.983395   \n",
      "\n",
      "         7         8   9   ...     26     27     28     29     30     31  \\\n",
      "0  1.865628  0.061517  62  ...  False  False   True  False  False  False   \n",
      "1  1.079128  0.061517  86  ...  False  False  False   True  False   True   \n",
      "2  0.554794  0.693086  64  ...  False  False  False   True  False  False   \n",
      "3 -0.231706 -1.291843  70  ...  False  False  False  False   True  False   \n",
      "4  1.341294  0.422414  69  ...  False  False  False   True  False  False   \n",
      "\n",
      "      32     33     34     35  \n",
      "0  False  False  False  False  \n",
      "1  False  False  False  False  \n",
      "2   True  False  False  False  \n",
      "3   True  False  False  False  \n",
      "4  False  False  False  False  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "SUCCESS: Data converted to float32.\n"
     ]
    }
   ],
   "source": [
    "# 1. Check what columns are causing the issue (Debugging)\n",
    "print(\"Checking for non-numeric columns...\")\n",
    "# Convert back to DataFrame just to see dtypes\n",
    "temp_df = pd.DataFrame(X_train)\n",
    "# Print columns that are of type 'object'\n",
    "print(temp_df.select_dtypes(include=['object']).head())\n",
    "\n",
    "# 2. THE FIX: Force conversion to float32\n",
    "# This will turn Booleans (True/False) into 1.0/0.0\n",
    "# And if there are strings like '1', it converts them. \n",
    "# If there are strings like 'Male', it will crash and tell us exactly which one is wrong.\n",
    "try:\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_val = X_val.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    y_val = y_val.astype(np.float32)\n",
    "    print(\"SUCCESS: Data converted to float32.\")\n",
    "except ValueError as e:\n",
    "    print(\"ERROR: You still have text in your data that cannot be converted!\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32106de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training V2 on cpu ---\n",
      "Epoch 1 | Loss: 0.6216 | Val AUC: 0.6856 (New Best!)\n",
      "Epoch 2 | Loss: 0.6084 | Val AUC: 0.6936 (New Best!)\n",
      "Epoch 5 | Loss: 0.6065 | Val AUC: 0.6944 (New Best!)\n",
      "Epoch 9 | Loss: 0.6058 | Val AUC: 0.6952 (New Best!)\n",
      "Epoch 10 | Loss: 0.6055 | Val AUC: 0.6946\n",
      "    -> Scheduler: Learning Rate reduced to 0.002500\n",
      "Epoch 14 | Loss: 0.6046 | Val AUC: 0.6958 (New Best!)\n",
      "Epoch 15 | Loss: 0.6044 | Val AUC: 0.6960 (New Best!)\n",
      "    -> Scheduler: Learning Rate reduced to 0.001250\n",
      "Epoch 20 | Loss: 0.6035 | Val AUC: 0.6953\n",
      "Epoch 23 | Loss: 0.6035 | Val AUC: 0.6962 (New Best!)\n",
      "Epoch 25 | Loss: 0.6032 | Val AUC: 0.6954\n",
      "Epoch 26 | Loss: 0.6032 | Val AUC: 0.6962 (New Best!)\n",
      "    -> Scheduler: Learning Rate reduced to 0.000625\n",
      "Epoch 30 | Loss: 0.6027 | Val AUC: 0.6964 (New Best!)\n",
      "Epoch 32 | Loss: 0.6025 | Val AUC: 0.6965 (New Best!)\n",
      "    -> Scheduler: Learning Rate reduced to 0.000313\n",
      "Epoch 35 | Loss: 0.6025 | Val AUC: 0.6962\n",
      "    -> Scheduler: Learning Rate reduced to 0.000156\n",
      "Epoch 40 | Loss: 0.6022 | Val AUC: 0.6961\n",
      "    -> Scheduler: Learning Rate reduced to 0.000078\n",
      "Epoch 45 | Loss: 0.6021 | Val AUC: 0.6962\n",
      "    -> Scheduler: Learning Rate reduced to 0.000039\n",
      "Epoch 50 | Loss: 0.6019 | Val AUC: 0.6961\n",
      "    -> Scheduler: Learning Rate reduced to 0.000020\n",
      "Final Best AUC: 0.6965\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 1. Define a \"Deeper & Wider\" Architecture\n",
    "class DiabetesNN_V2(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DiabetesNN_V2, self).__init__()\n",
    "        \n",
    "        # Layer 1: Input -> 128 (Wider)\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.2) # Lower dropout slightly to allow more learning\n",
    "        \n",
    "        # Layer 2: 128 -> 64\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        # Layer 3: 64 -> 32\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.dropout3 = nn.Dropout(0.1) # Very low dropout before output\n",
    "        \n",
    "        # Layer 4: 32 -> 1\n",
    "        self.output = nn.Linear(32, 1)\n",
    "        \n",
    "        # Activation: LeakyReLU (Slope 0.01 for negative values)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.leaky_relu(self.bn1(self.layer1(x))))\n",
    "        x = self.dropout2(self.leaky_relu(self.bn2(self.layer2(x))))\n",
    "        x = self.dropout3(self.leaky_relu(self.bn3(self.layer3(x))))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "# 2. Re-Initialize\n",
    "model = DiabetesNN_V2(input_dim=X_train.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# FIX: Remove 'verbose=True'\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "# 3. Training Loop with Scheduler\n",
    "epochs = 50 \n",
    "batch_size = 2048 \n",
    "\n",
    "print(f\"--- Training V2 on {device} ---\")\n",
    "best_auc = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    permutation = torch.randperm(X_train_tensor.size()[0])\n",
    "    for i in range(0, X_train_tensor.size()[0], batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = X_train_tensor[indices].to(device), y_train_tensor[indices].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(X_val_tensor.to(device)).cpu().numpy()\n",
    "        val_auc = roc_auc_score(y_val, val_preds)\n",
    "    \n",
    "    # Update Scheduler\n",
    "    before_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(val_auc)\n",
    "    after_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Print progress\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        print(f\"Epoch {epoch+1} | Loss: {epoch_loss/(len(X_train)//batch_size):.4f} | Val AUC: {val_auc:.4f} (New Best!)\")\n",
    "    elif (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {epoch_loss/(len(X_train)//batch_size):.4f} | Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "    # Manual Verbose Check: If LR changed, print it\n",
    "    if before_lr != after_lr:\n",
    "        print(f\"    -> Scheduler: Learning Rate reduced to {after_lr:.6f}\")\n",
    "\n",
    "print(f\"Final Best AUC: {best_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d281bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data converted to float32.\n",
      "submission.csv saved! Ready for upload.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare the Kaggle Test Data\n",
    "# Apply the exact same fix we used on X_train\n",
    "try:\n",
    "    X_kaggle_test = X_kaggle_test.astype(np.float32)\n",
    "    print(\"Test data converted to float32.\")\n",
    "except ValueError:\n",
    "    print(\"Error converting test data.\")\n",
    "\n",
    "# 2. Convert to Tensor\n",
    "X_test_tensor = torch.tensor(X_kaggle_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# 3. Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get raw probabilities\n",
    "    test_predictions = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# 4. Create Submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'diagnosed_diabetes': test_predictions.flatten()\n",
    "})\n",
    "\n",
    "# 5. Save\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"submission.csv saved! Ready for upload.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
